# vit
Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at the following resolutions:

## Models
-  resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224, patch 16x16, base [vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224)
-  resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224, patch 16x16, large [vit-large-patch16-224](https://huggingface.co/google/vit-large-patch16-224)
- resolution 224x224, patch 16x16, base [vit-base-patch16-224-in21k](https://huggingface.co/google/vit-base-patch16-224-in21k)
- resolution 224x224, patch 16x16, huge [vit-huge-patch14-224-in21k](https://huggingface.co/google/vit-huge-patch14-224-in21k)
- resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384, patch 16x16, base [vit-base-patch16-384](https://huggingface.co/google/vit-base-patch16-384)
- resolution 384x384, patch 32x32, large [vit-large-patch32-384](https://huggingface.co/google/vit-large-patch32-384)
- resolution 384x384, patch 32x32, base[google/vit-base-patch32-384](https://huggingface.co/google/vit-base-patch32-384)

